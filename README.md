## Image_captioning

# Overview
This work deploys a state-of-the-art image captioning model employing Vision Transformers (ViT) for feature extraction and GPT-2 for captioning. Classical CNN+LSTM-based architectures fail to address long-range dependencies and contextual consistency. With the use of transformer-based architectures, our solution dramatically enhances caption quality and contextual precision.

# Installation
To run the project, install the necessary dependencies:

git clone https://github.com/amit9594/image_captioning.git
cd image_captioning

# Future Work
Try bigger transformer models such as GPT-4
Enhance data augmentation methods for robust learning
Optimize for real-time applications

# Contributing
Pull requests are welcome! If youâ€™d like to contribute, please fork the repository and submit a PR.
